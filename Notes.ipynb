{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are my notes and little tests while learning Google's machine learning library, TensorFlow.\n",
    "\n",
    "TensorFlow can now only be installed in Mac OS and Linux. I'm using virtual machine running Ubuntu on Windows 10.\n",
    "\n",
    "First one are some basic usage of TensorFlow.\n",
    "\n",
    "The second is a \"multinomial logistic regression\".\n",
    "\n",
    "The third part is more thorough with a little test.\n",
    "\n",
    "Finally, the last one is a \"convolutional neural network\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are some basic usage of TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12.]]\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "[array([ 3.], dtype=float32), array([ 9.], dtype=float32)]\n",
      "[array([ 15.], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def Session():\n",
    "    matrix_1 = tf.constant([[3.0, 3.0]]) #1x2 matrix\n",
    "    matrix_2 = tf.constant([[2.0], [2.0]]) #2x1 matrix\n",
    "    product = tf.matmul(matrix_1, matrix_2)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        print(sess.run(product))\n",
    "\n",
    "def Varialbes():\n",
    "    state = tf.Variable(0)\n",
    "    one = tf.constant(1)\n",
    "    new_state = tf.add(state, one)\n",
    "    update = tf.assign(state, new_state)\n",
    "    init = tf.initialize_all_variables()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        print(sess.run(state))\n",
    "        for i in range(3):\n",
    "            sess.run(new_state)\n",
    "            sess.run(update)\n",
    "            print(sess.run(state))\n",
    "\n",
    "def Fetches():\n",
    "    one = tf.constant([1.0])\n",
    "    two = tf.constant([2.0])\n",
    "    three = tf.constant([3.0])\n",
    "    sum = tf.add(one, two)\n",
    "    product = tf.mul(sum, three)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        print(sess.run([sum, product]))\n",
    "\n",
    "def Feeds():\n",
    "    input_1 = tf.placeholder(tf.float32)\n",
    "    input_2 = tf.placeholder(tf.float32)\n",
    "    output = tf.mul(input_1, input_2)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        print(sess.run([output], feed_dict={input_1:[3.0], input_2:[5.0]}))\n",
    "\n",
    "Session()\n",
    "Varialbes()\n",
    "Fetches()\n",
    "Feeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a quick implementaion of multinomial logistic regression used in MNIST.\n",
    "\n",
    "MNIST is a simple computer vision dataset. It consists of images of handwritten digits.\n",
    "\n",
    "We're going to train the model to look at images and predict what digits they are.\n",
    "\n",
    "The accuracy is around 90%. It's very bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "The accuracy is 0.92\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "W = tf.Variable(tf.zeros([784, 10])) # x * W = None * 10\n",
    "b = tf.Variable(tf.zeros([10])) # 1 * 10\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b) # probability distribution (0 < y < 1)\n",
    "\n",
    "y_ = tf.placeholder(tf.float32, [None, 10]) # true distribution (one-hot vector)\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=1))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(1000):\n",
    "        batch_x, batch_y = mnist.train.next_batch(100)\n",
    "        sess.run(train_step, feed_dict={x: batch_x, y_: batch_y})\n",
    "    correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) # argmax: return the largest value of a tensor, dimension to reduce\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # cast: change data type to float32\n",
    "    print('The accuracy is %.2f' % sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a thorough implementation of multinomial logistic regression alse used in MNIST.\n",
    "\n",
    "However, this time I draw three 7 * 7 matrices which is reshaped to 28 * 28 and let the model to guess the numbers.\n",
    "\n",
    "The result isn't quite accurate, but I'm satisfied with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "input:\n",
      "[[0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0]\n",
      " [0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0]\n",
      " [0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0]\n",
      " [0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0]\n",
      " [0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0]\n",
      " [0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0]\n",
      " [0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0]\n",
      " [0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0]]\n",
      "This is 3.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10]) # one-hot vector, label\n",
    "W = tf.Variable(tf.zeros([784,10])) # 784 inputs and 10 outputs\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.nn.softmax(tf.matmul(x, W) + b)\n",
    "sess.run(tf.initialize_all_variables())\n",
    "\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "for i in range(1000):\n",
    "    batch = mnist.train.next_batch(100)\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1]}) # replace placeholder wdwith training samples, x: handwriting, y_: answer\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "test0 = np.array([0,0,0,0,0,0,0,\n",
    "                  0,0,1,1,1,0,0,\n",
    "                  0,1,0,0,0,1,0,\n",
    "                  0,1,0,0,0,1,0,\n",
    "                  0,1,0,0,0,1,0,\n",
    "                  0,0,1,1,1,0,0,\n",
    "                  0,0,0,0,0,0,0])\n",
    "\n",
    "test1 = np.array([0,0,0,1,0,0,0,\n",
    "                  0,0,0,1,0,0,0,\n",
    "                  0,0,0,1,0,0,0,\n",
    "                  0,0,0,1,0,0,0,\n",
    "                  0,0,0,1,0,0,0,\n",
    "                  0,0,0,1,0,0,0,\n",
    "                  0,0,0,1,0,0,0])\n",
    "\n",
    "test3 = np.array([0,0,1,1,1,0,0,\n",
    "                  0,1,0,0,0,1,0,\n",
    "                  0,0,0,0,0,1,0,\n",
    "                  0,0,1,1,1,0,0,\n",
    "                  0,0,0,0,0,1,0,\n",
    "                  0,1,0,0,0,1,0,\n",
    "                  0,0,1,1,1,0,0])\n",
    "\n",
    "def drawer(test):\n",
    "    sketch_1 = []\n",
    "    sketch_2 = []\n",
    "    picture_1 = []\n",
    "    picture_2 = []\n",
    "    for i in test:\n",
    "        for j in range(4):\n",
    "            sketch_1.append(i)\n",
    "    for k in range(7):\n",
    "        sketch_2.append(sketch_1[k*28:k*28+28])\n",
    "    for l in range(len(sketch_2)):\n",
    "        for m in range(4):\n",
    "            picture_1.append(sketch_2[l])\n",
    "    print('input:')\n",
    "    print(np.array(picture_1))\n",
    "    for n in picture_1:\n",
    "        for o in n:\n",
    "            picture_2.append(o)\n",
    "    return picture_2\n",
    "\n",
    "label = [[1,0,0,0,0,0,0,0,0,0],[0,1,0,0,0,0,0,0,0,0],[0,0,1,0,0,0,0,0,0,0],[0,0,0,1,0,0,0,0,0,0],\n",
    "         [0,0,0,0,1,0,0,0,0,0],[0,0,0,0,0,1,0,0,0,0],[0,0,0,0,0,0,1,0,0,0],[0,0,0,0,0,0,0,1,0,0],\n",
    "         [0,0,0,0,0,0,0,0,1,0],[0,0,0,0,0,0,0,0,0,1]]\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1)) # argmax(): return the index of the highest entry along some axis\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "test = drawer(test3)\n",
    "for n in label:\n",
    "    if accuracy.eval(feed_dict={x: [test], y_: [n]}) == 1:\n",
    "        print('This is %d.' % label.index(n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one is a simple convolutional neural network.\n",
    "\n",
    "I finally get to the deep learning.\n",
    "\n",
    "Those comments are my notes. Ignore it or read it all!\n",
    "\n",
    "It's trained by MNIST, too.\n",
    "\n",
    "However, the highest accuracy can get to 100%!\n",
    "\n",
    "With a lot, a lot of time......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method BaseSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x7f9e28b37ba8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 171, in __del__\n",
      "    self.close()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\", line 976, in close\n",
      "    self._default_session.__exit__(None, None, None)\n",
      "  File \"/usr/lib/python3.5/contextlib.py\", line 66, in __exit__\n",
      "    next(self.gen)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\", line 3378, in get_controller\n",
      "    % type(default))\n",
      "AssertionError: Nesting violated for default stack of <class 'weakref'> objects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.14\n",
      "step 100, training accuracy 0.84\n",
      "step 200, training accuracy 0.94\n",
      "step 300, training accuracy 0.88\n",
      "step 400, training accuracy 0.98\n",
      "step 500, training accuracy 0.9\n",
      "step 600, training accuracy 1\n",
      "step 700, training accuracy 0.96\n",
      "step 800, training accuracy 0.86\n",
      "step 900, training accuracy 1\n",
      "step 1000, training accuracy 0.94\n",
      "step 1100, training accuracy 1\n",
      "step 1200, training accuracy 0.98\n",
      "step 1300, training accuracy 1\n",
      "step 1400, training accuracy 0.98\n",
      "step 1500, training accuracy 0.96\n",
      "step 1600, training accuracy 0.98\n",
      "step 1700, training accuracy 0.96\n",
      "step 1800, training accuracy 0.96\n",
      "step 1900, training accuracy 1\n",
      "step 2000, training accuracy 0.94\n",
      "step 2100, training accuracy 1\n",
      "step 2200, training accuracy 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-936395abd8ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[0mtrain_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"step %d, training accuracy %g\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m     \u001b[0mtrain_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"test accuracy %g\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmnist\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m   1549\u001b[0m         \u001b[0mnone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[0m_run_using_default_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[1;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   3531\u001b[0m                        \u001b[1;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3532\u001b[0m                        \"graph.\")\n\u001b[1;32m-> 3533\u001b[1;33m   \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3535\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 372\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    373\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 636\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    637\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m       \u001b[1;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    706\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 708\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    709\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    713\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 715\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    716\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    695\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m    696\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 697\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "##############################################################\n",
    "'''Train and Evaluate the Model'''                           #\n",
    "from tensorflow.examples.tutorials.mnist import input_data   #\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)#\n",
    "                                                             #\n",
    "sess = tf.InteractiveSession()                               #\n",
    "                                                             #\n",
    "x = tf.placeholder(tf.float32, shape=[None, 784])            #\n",
    "y_ = tf.placeholder(tf.float32, shape=[None, 10])            #\n",
    "sess.run(tf.initialize_all_variables())                      #\n",
    "##############################################################\n",
    "\n",
    "# Initialize weights with a little noise for symmetry breaking\n",
    "# and preventing 0 gradients.\n",
    "def weight_variable(shape):\n",
    "    # Outputs random values from a truncated normal distribution.\n",
    "    initial = tf.truncated_normal(shape=shape, stddev=0.1) # standard deviation\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# Initialize biases with slightly positive numbers to avoid \"dead neurons\".\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(value=0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# Computes a 2D convolution from 4D input and filter tensors.\n",
    "def conv2d(x, W):\n",
    "    # input: [batch, in_height, in_width, in_channels]\n",
    "    # filter: [filter_height, filter_width, in_channels, out_channels]\n",
    "    # 1. Flattens the filter to a 2D matrix with shape [filter_height * filter_width * in_channels, output_channels].\n",
    "    # 2. Extracts image patches from the input tensor to form a tensor of shape [batch, out_height, out_width, filter_height * filter_width * in_channels].\n",
    "    # 3. output = 2D filter * image patch\n",
    "    # strides: a list of ints, 1D, length 4\n",
    "    # ????????????????????????????????????????????????????????????????????????????????????????????????????\n",
    "    return tf.nn.conv2d(input=x, filter=W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    # value: a 4D tensor with shape [batch, height, width, channels]\n",
    "    # ksize: a list of ints that has length >= 4\n",
    "    # strides:  a list of ints that has length >= 4\n",
    "    return tf.nn.max_pool(value=x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "'''First Convolutional Layer: convolution >> max pooling'''\n",
    "# 32 features for each 5x5 patch\n",
    "W_conv_1 = weight_variable([5, 5, 1, 32]) # [height, width, input channels, output channels]\n",
    "b_conv_1 = bias_variable([32])\n",
    "# Reshape x to 28 * 28.\n",
    "# 1: number of color channels\n",
    "# -1: Total size remains constant.\n",
    "x_image = tf.reshape(x, [-1,28,28,1])\n",
    "# relu: rectified linear unit\n",
    "# rectified linear: an activation function defined as f(x) = max(0, x)\n",
    "# Its smooth approximation is the analytic function defined as f(x) = ln(1 + e^x) (softplus)\n",
    "h_conv_1 = tf.nn.relu(conv2d(x_image, W_conv_1) + b_conv_1)\n",
    "h_pool1 = max_pool_2x2(h_conv_1)\n",
    "\n",
    "'''Second Convolutional Layer: convolution >> max pooling'''\n",
    "# 64 features for each 5x5 patch\n",
    "W_conv_2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv_2 = bias_variable([64])\n",
    "h_conv_2 = tf.nn.relu(conv2d(h_pool1, W_conv_2) + b_conv_2)\n",
    "h_pool_2 = max_pool_2x2(h_conv_2)\n",
    "\n",
    "'''Densily Connected Layer: Add 1024 fully-connected neurons to process the entire image.'''\n",
    "# Image size is 7 * 7 now.\n",
    "W_fc_1 = weight_variable([7 * 7 * 64, 1024])\n",
    "b_fc_1 = bias_variable([1024])\n",
    "# Reshape the tensor from the pooling layer into a batch of vectors.\n",
    "h_pool_2_flat = tf.reshape(h_pool_2, [-1, 7*7*64])\n",
    "h_fc_1 = tf.nn.relu(tf.matmul(h_pool_2_flat, W_fc_1) + b_fc_1)\n",
    "\n",
    "'''Drop-out Layer: To reduce overfitting.'''\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "h_fc_1_drop = tf.nn.dropout(h_fc_1, keep_prob)\n",
    "\n",
    "'''Readout Layer: Softmax'''\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "y_conv = tf.nn.softmax(tf.matmul(h_fc_1_drop, W_fc2) + b_fc2)\n",
    "\n",
    "'''Train and Evaluate the Model'''\n",
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv), reduction_indices=[1]))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "sess.run(tf.initialize_all_variables())\n",
    "for i in range(20000):\n",
    "    batch = mnist.train.next_batch(50)\n",
    "    if i % 100 == 0:\n",
    "        train_accuracy = accuracy.eval(feed_dict={x: batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "        print(\"step %d, training accuracy %g\" % (i, train_accuracy))\n",
    "    train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "print(\"test accuracy %g\" % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels, keep_prob: 1.0}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}